"""RAG system using LangChain."""
import tempfile
import os
from typing import Optional

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

from core.config import settings


class RAGSystem:
    """RAG system for querying PDF documents."""
    
    def __init__(self):
        """Initialize RAG system."""
        self.embeddings = OpenAIEmbeddings(
            openai_api_key=settings.openai_api_key,
            model=settings.embedding_model
        )
        self.vector_store: Optional[FAISS] = None
        self.llm = ChatOpenAI(
            openai_api_key=settings.openai_api_key,
            model=settings.openai_model,
            temperature=0.7,
            max_tokens=1000
        )
        self.qa_chain: Optional[RunnablePassthrough] = None
        
        # Create prompt template
        self.prompt_template = ChatPromptTemplate.from_messages([
            ("system", "You are a helpful assistant that answers questions based on the provided context from a PDF document. Answer the question based only on the context provided. If the context doesn't contain enough information to answer the question, say so. Be concise and accurate."),
            ("human", "Context:\n{context}\n\nQuestion: {question}")
        ])
    
    def process_pdf(self, pdf_bytes: bytes) -> int:
        """
        Process a PDF file and create vector store.
        
        Args:
            pdf_bytes: PDF file content as bytes
            
        Returns:
            Number of chunks created
        """
        # Save to temporary file for PyPDFLoader
        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
            tmp_file.write(pdf_bytes)
            tmp_path = tmp_file.name
        
        try:
            # Load PDF using LangChain
            loader = PyPDFLoader(tmp_path)
            documents = loader.load()
            
            # Split into chunks
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=settings.chunk_size,
                chunk_overlap=settings.chunk_overlap,
                length_function=len,
                separators=["\n\n", "\n", " ", ""]
            )
            chunks = text_splitter.split_documents(documents)
            
            # Create vector store
            self.vector_store = FAISS.from_documents(
                documents=chunks,
                embedding=self.embeddings
            )
            
            # Initialize RAG chain
            self._initialize_chain()
            
            return len(chunks)
        finally:
            # Clean up temporary file
            if os.path.exists(tmp_path):
                os.unlink(tmp_path)
    
    def _initialize_chain(self):
        """Initialize the RAG chain."""
        if self.vector_store is None:
            return
        
        retriever = self.vector_store.as_retriever(search_kwargs={"k": 5})
        
        def format_docs(docs):
            return "\n\n".join(doc.page_content for doc in docs)
        
        self.qa_chain = (
            {
                "context": retriever | format_docs,
                "question": RunnablePassthrough()
            }
            | self.prompt_template
            | self.llm
            | StrOutputParser()
        )
    
    def query(self, question: str) -> str:
        """
        Answer a question using RAG.
        
        Args:
            question: User question
            
        Returns:
            Answer generated by the model
        """
        if self.qa_chain is None:
            return "No PDF processed yet. Please upload a PDF first."
        
        try:
            result = self.qa_chain.invoke(question)
            return result
        except Exception as e:
            return f"Error processing query: {str(e)}"

